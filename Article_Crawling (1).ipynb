{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a4c739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "Collecting urllib3[secure,socks]~=1.26\n",
      "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
      "Requirement already satisfied: cryptography>=1.3.4; extra == \"secure\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2.9.2)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14; extra == \"secure\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (19.1.0)\n",
      "Requirement already satisfied: certifi; extra == \"secure\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2020.6.20)\n",
      "Requirement already satisfied: idna>=2.0.0; extra == \"secure\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting async-generator>=1.10\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (19.3.0)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.0)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cryptography>=1.3.4; extra == \"secure\"->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Installing collected packages: urllib3, h11, wsproto, outcome, async-generator, sniffio, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "Successfully installed async-generator-1.10 h11-0.13.0 outcome-1.2.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.10 wsproto-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: requests 2.24.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.10 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1279 sha256=bab1f873ef1366b101a059621990704e2ef8ccc36ab47c373b2ed11f5498da1e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.47.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\anaconda3\\lib\\site-packages (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install bs4\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf420458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_585520\\3692721638.py:13: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "year = ['2022'] # ['2020', '2021']\n",
    "press_type = ['조선일보', '중앙일보', '동아일보', '경향신문', '한겨레', 'KBS', 'MBC', 'SBS'] # ['조선일보', '중앙일보', '동아일보', '경향신문', '한겨레', 'KBS', 'MBC', 'SBS']\n",
    "save_path = './data/'\n",
    "base_path = './url/'\n",
    "chrome_options = webdriver.ChromeOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569d070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(path, press_type):\n",
    "    df = pd.read_excel(path, sheet_name = 'sheet')\n",
    "#     df = pd.read_csv(path, encoding='utf-8')\n",
    "    url = df[df['언론사'] == press_type]['URL'].to_list()\n",
    "    return url\n",
    "\n",
    "def chosun_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h1', class_='article-header__headline | font--secondary text--black').getText()\n",
    "    body = bs.find('section', class_='article-body').getText()\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body\n",
    "\n",
    "def joongang_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h1', class_='headline').getText().replace('\\n', \"\")\n",
    "    body = bs.find('div', class_='article_body fs3').findAll('p')\n",
    "    body_text = \"\"\n",
    "    for i in body:\n",
    "        body_text += i.getText()\n",
    "        body_text = re.sub(\"  \", \"\", body_text)\n",
    "        body_text = body_text.replace('\\n', \"\")\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body_text\n",
    "\n",
    "def donga_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h1', class_='title').getText()\n",
    "    body = bs.find_all('div', {\"itemprop\": \"articleBody\"})\n",
    "    body_text = \"\"\n",
    "    for i in body:\n",
    "        body_text += i.getText()\n",
    "    body_text = body_text.replace('\\n', '')\n",
    "    body_text = body_text.rstrip('Copyright ⓒ 동아일보 & donga.com')\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body_text\n",
    "\n",
    "def hangr_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('span', class_='title').getText()\n",
    "    body = bs.find('div', class_='text')\n",
    "    if body.find('div', class_='showPic on') is not None:\n",
    "        body.find('div', class_='showPic on').decompose()\n",
    "    body = body.getText()\n",
    "    if '\\n' in str(body):\n",
    "        body = body.replace(\"\\n\", \"\")\n",
    "    if '\\r' in str(body):\n",
    "        body = body.replace(\"\\r\", \"\")\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body\n",
    "\n",
    "def gyunghyang_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 5))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h1', class_='headline').getText()\n",
    "    body = bs.find_all('p', class_='content_text')\n",
    "    body_text = \"\"\n",
    "    for i in body:\n",
    "        body_text += i.getText()\n",
    "    if '\\n' in str(body_text):\n",
    "        body_text = body_text.replace(\"\\n\", \"\")\n",
    "    if '\\r' in str(body_text):\n",
    "        body_text = body_text.replace(\"\\r\", \"\")\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 5))\n",
    "    return title + ' ' + body_text\n",
    "\n",
    "def kbs_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h5', class_='tit-s').getText()\n",
    "    body = bs.find('div', class_='detail-body font-size')\n",
    "    for i in body.findAll('center'):\n",
    "        i.decompose()\n",
    "    body = body.getText()\n",
    "    if '\\n' in str(body):\n",
    "        body = body.replace(\"\\n\", \"\")\n",
    "    if '\\r' in str(body):\n",
    "        body = body.replace(\"\\r\", \"\")\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body\n",
    "\n",
    "def sbs_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h3', id='vmNewsTitle').getText()\n",
    "    body = bs.find('div', class_='text_area').getText()\n",
    "    if '\\n' in str(body):\n",
    "        body = body.replace(\"\\n\", \"\")\n",
    "    if '\\r' in str(body):\n",
    "        body = body.replace(\"\\r\", \"\")\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body\n",
    "\n",
    "def mbc_crawling(url):\n",
    "    current_handle = driver.current_window_handle\n",
    "    driver.get(url)\n",
    "    tabs = driver.window_handles\n",
    "    if len(tabs) != 1:\n",
    "        for handle in tabs:\n",
    "            if handle != current_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                driver.implicitly_wait(10)\n",
    "                driver.close()\n",
    "    driver.switch_to.window(current_handle)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    req = driver.page_source\n",
    "    bs = BeautifulSoup(req, 'lxml')\n",
    "    title = bs.find('h2', class_='art_title').getText()\n",
    "    body = bs.find('div', class_='news_txt').getText()\n",
    "    if '\\n' in str(body):\n",
    "        body = body.replace(\"\\n\", \"\")\n",
    "    if '\\r' in str(body):\n",
    "        body = body.replace(\"\\r\", \"\")\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(random.randrange(1, 3))\n",
    "    return title + ' ' + body\n",
    "\n",
    "def save(save_path, data):\n",
    "    with open(save_path, 'w', encoding='UTF-8') as f:\n",
    "        cnt = 1\n",
    "        for i in data:\n",
    "            f.write(str(cnt) + \" \" + i + '\\n')\n",
    "            cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e55347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# press_type = ['조선일보', '중앙일보', '동아일보', '경향신문', '한겨레', 'KBS', 'MBC', 'SBS']\n",
    "# methods: chosun_crawling, joongang_crawling, donga_crawling, hangr_crawling, \n",
    "#gyunghyang_crawling, kbs_crawling, sbs_crawling, mbc_crawling\n",
    "def run(path,keyword):\n",
    "    start_time = time.time()\n",
    "    text_list = []\n",
    "    press = '한겨레' # Change news agency\n",
    "#     path = base_path + y + '_' + str(m) + '.csv'\n",
    "    url = get_url(path, press_type=press)\n",
    "    cnt = 0\n",
    "    for i in tqdm(url[:2]):\n",
    "#     for i in tqdm(url):\n",
    "        try:\n",
    "            text_list.append(hangr_crawling(i)) # Changing crawling function\n",
    "        except Exception as e:\n",
    "            print(\"Error \" + press + \" in Index \"+str(cnt), e)\n",
    "            cnt+=1\n",
    "            pass\n",
    "        cnt += 1\n",
    "    s_path = save_path + press +'_' + keyword + '.txt'\n",
    "    save(s_path, text_list)\n",
    "    running_time = (time.time() - start_time) / 60\n",
    "    print(\"running_time: %.2f minutes\" % running_time)\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8804c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_585520\\3303788108.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver.exe')\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_time: 0.55 minutes\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('./chromedriver.exe')  \n",
    "# chrome_options.binary_location = \"C:/Program Files/Google/Chrome Beta/Application/chrome.exe\"\n",
    "# driver = webdriver.Chrome('./chromedriver_win32/chromedriver.exe', options=chrome_options)\n",
    "driver.implicitly_wait(5)\n",
    "start_time = time.time()\n",
    "keyword = '야구'\n",
    "path = base_path + 'NewsResult_20230126-20230426.xlsx'\n",
    "run(path, keyword)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd2c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
